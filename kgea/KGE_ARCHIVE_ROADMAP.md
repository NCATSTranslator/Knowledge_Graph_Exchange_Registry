# KGE Archive Roadmap

This document is serving as a general focal point for planning the implementation of the Knowledge Graph Exchange Archive system ("Archive"), the general architectural vision for which is discussed in [the high level architectural vision document](../KGE_ARCHIVE_ARCHITECTURE.md).  

In discussing the plan, the 'primary' client pertains to the initial user transaction with the Archive of uploading the original KGE File Set to the Archive. A 'secondary' client pertains to a second user activity (fo the same, or different, user) of accessing that KGE File Set, after it has been uploaded to the Archive.

Further specific details and road map of the Archive's design and its implementation - in global terms and with respect to the numbered functional parts noted on the KGE Archive Component Architecture diagram are summarized here below in this document .

- [General Considerations](#general-considerations)
1. [Primary Client KGE File Set Upload](#1-primary-client-kge-file-set-upload)
2. [KGE Archive Server](#2-kge-archive-server)
3. [KGE File Set Network Storage](#3-kge-file-set-network-storage)
4. [KGX Metadata Generation](#4-kgx-metadata-generation)
5. [KGE SmartAPI Registration](#5-kge-smartapi-registration)
6. [Secondary Client Access to KGE SmartAPI Entries](#6-secondary-client-access-to-kge-smartapi-entries)
7. [Secondary Client Access to KGE Files](#7-secondary-client-access-to-kge-files)

![KGE Archive Architecture](../docs/KGE_Archive_Architecture.png?raw=true "KGE Archive Architecture")

# General Considerations

- The heart of the KGE Archive system will be a publicly visible, user authentication secured, web service application [2].
- Since NCATS (Biomedical Translator Consortium) operates most of its deployed infrastructure on a private (VPN) leased sub-cloud of Amazon Web Services (AWS) servers and related components, deployment of the KGI Archive is assumed to target AWS EC2, and related PAAS services, as its deployment platform.

# 1. Primary Client KGE File Set Upload

The precise form, protocol and software support for KGE File Set uploading [1] is to be elaborated...

# 2. KGE Archive Server

- The core web service application [2] will be installed to run within a suitable software deployment framework (a Docker Compose managed set of Docker containers) hosted on an AWS EC2 instance, running a common flavor (i.e. Ubuntu) of the Linux operating system.
  

- **Web Services Specification:** a current release of the OpenAPI 3 web services specification standard is being used to specify the web services application programming interface (API) of the Archive. [OpenAPI Tools code generation](https://github.com/openapitools/openapi-generator-cli) is used to convert the API into stub server code for elaboration. This tool is wrapped in a pair of (bash) shell scripts, run to further automate and parameterize the server code generation process.

- **Primary implementation language:** for Archive software components will be a recent release (3.9++) of Python. Related off-the-shelf standards and libraries are being used to develop the required components:

    - **Build & Dependency Management:** the application configuration Python dependency management and build process will be managed by the Python `pipenv` tool;
      
    - **Web Application Framework:** the Python Flask, with customized business logic serving handlers to web service API code stubs as generated by the aforementioned OpenAPI code generator tool;
      
    - **Amazon Web Services:** the [latest release of the available Python AWS Software Development Kit (Boto3)](https://aws.amazon.com/sdk-for-python/) will be leveraged to integrate the web services application with AWS infrastructure.
  
    - **Github Transactions:** publication of SmartAPI entries for KGE File Sets [5] can be accomplished using an available [Python library for programmatic access to Github](https://docs.github.com/en/rest/overview/libraries#python). 
   
# 3. KGE File Set Network Storage

Archived KGE file sets encoding Translator Knowledge Graphs, are KGX text files which are anticipated to be, on average, fairly large (e.g. gigabytes in size). Although such files could potentially be hosted on AWS EBS volumes attached to the aforementioned EC2 server instance, longer term stable persistence of such large files only periodically accessed, suggests that they should be hosted within an AWS S3 bucket. The aforementioned Boto3 Python AWS SDK library will (hopefully) efficiently broker web service application transactions with S3.
   
# 4. KGX Metadata Generation

KGX-generated 'content' metadata, perhaps augmented with some additional KGE Archive-specific 'provider' metadata, will need to be associated with every distinct knowledge graph published as a KGE File Set.

Two options are possible for the provision of KGX 'content' metadata:

1. the primary client will pre-generate a KGX content metadata file for upload alongside the KGX files forming the KGE File Set;

2. the Archive server will run an instance of KGX (running as a background process on the server, perhaps in a dedicated Docker container?) to generate the required content metadata file from previously uploaded KGX files. 
   
# 5. KGE SmartAPI Registration

The standard mechanism for the recording of Translator SmartAPI Registry ("Registry") entries to Translator resources to consume SmartAPI-compliant OpenAPI 3 YAML specifications from a designated Github repository location (e.g. the [Translator API Registry github repository](https://github.com/NCATS-Tangerine/translator-api-registry)).  Such a location can contain multiple distinct SmartAPI entries, one per distinct API.  Every API registered in such a location needs to resolve to a host/path location of a live OpenAPI 3 implementation of the API.

After uploading KGE File Set (and adding KGX 'content' metadata [4] if required), the Archive needs to publish such a SmartAPI entry to the Translator and make provisions for the live hosting of the API.

For the former objective, the Archive will populate a KGE File Set specific instance of a [KGE File Set SmartAPI specification template](../api/kge_smartapi_entry.yaml). For the latter objective, the resulting KGE SmartAPI YAML file will actually resolve to a KGE File Set associated subpath on the Archive, as programmatically defined in the full [Archive API web services OpenAPI specification](../api/kgea_api.yaml) and its implementation, described herein.

Archive publication of KGE File Set SmartAPI entries will consist of a `git` commit and push of the KGE File Set-specific API YAML file to a  SmartApi registered Github repository location, likely within the [Translator API Registry github repository](https://github.com/NCATS-Tangerine/translator-api-registry) and updating of any associated [API List](https://github.com/NCATS-Tangerine/translator-api-registry/blob/master/API_LIST.yml) on the site. The full details of full automation of this process require further elaboration.

# 6. Secondary Client Access to KGE SmartAPI Entries

As mentioned in [the high level architectural vision document](../KGE_ARCHIVE_ARCHITECTURE.md), the SmartAPI Registry team already have infrastructure in place on the [Translator SmartAPI Portal](https://smart-api.info/portal/translator) to handle this step in the KGE workflow, so there is no action required here. 
   
# 7. Secondary Client Access to KGE Files

As noted in [5] above, KGE File Set SmartAPI entries will point to a live server path indexed by knowledge graph name and hosted by the Archive web service application [2].  The key design considerations for this step is to specify the specific modality of access and its practical implementation.

With respect to modality, one specific REST path definition in the [Archive API web services OpenAPI specification](../api/kgea_api.yaml) is defined to handle file retrieval as a some kind of URL brokered access to the files. How this URL is to be accessed remains to be further elaborated.

In terms of practical implementation, given the anticipated large anticipated size of many KGE File Sets, technical options for downloading or streaming such files will require additional consideration. Two general ideas come to mind at the moment:

1. That the Archive web server acts like a proxy gateway streaming such files to a secondary client, through the server, from the back end network storage system (AWS S3 bucket, perhaps via EBS buffering on the EC2 server)

2. That the Archive merely returns a direct URL - with an appropriate user authentication/authorization access token (OAuth2, with short term expiration?) - pointing to the back end network storage system (AWS S3 bucket), which the secondary client then uses to directly access the files from the storage system (without going through the server).

